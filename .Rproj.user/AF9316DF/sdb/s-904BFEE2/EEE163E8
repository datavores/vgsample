{
    "contents" : "---\ntitle: \"UVList Data Cleaning\"\nauthor: \"Brian Waismeyer\"\ndate: \"`r Sys.Date()`\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{UVList Data Cleaning}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\nThis document covers the details of how the scraped UVList records are processed\nfor merging. It starts by loading the raw scraped data (see the UVList Data \nScraping document for details) and concludes with UVList records in the desired \nstructure for merging.\n\nThe UVList data were retrieved on 2/1/2016 and consist of all the game records \navailable on the UVList game search pages at that time.\n\n**Note**: Carefully observe the globals (e.g.,`id_variations_from_scratch`) in \nthe opening code below. The major processes underlying the identification of \nvalue mispellings/variations in the game records (especially the game titles) \nare _very_ lengthy (up to several hours) and resolving identified assocations is\noften best informed by some additional manual interaction with the data to \nassess if new auto-resolve rules are appropriate. Unless you are very certain \nthe data need to be re-processed, it is suggested that you keep globals at their\ndefaults and work with the intermediate data products loaded into the workspace.\n\n**Note**: If run, this document assumes certain resources (e.g., the UVList raw\ndata .Rds) are available in the local working directory.\n\n## Prepare the Workspace\n\nWe start by loading supporting packages and setting desired global options.\n\n```{r prepare_workspace}\n# Load supporting packages.\nlibrary(dplyr)          # Data manipulation.\nlibrary(stringr)        # String manipulation.\nlibrary(stringdist)     # Fuzzy string matching.\n\n# Set document globals.\nrun_from_scratch <- FALSE\n```\n\nTo keep the later portions of the document concise, we also define our custom\nhelper functions here. Functions are presented in the order they are called.\n\n```{r define_helper_functions}\nsource(\"./cleaning_functions.R\")\n```\n\nFinally, we load the raw UVList data.\n\n```{r load_raw_data}\n# Load the raw results of the UVList scraping script. This is a list of \n# game-by-system tables (i.e., games will have as many entries as they have\n# systems recorded as associated with them).\nload(\"./uvlist_raw.Rds\")\n\n# If not running from scratch, we also load all other intermediate products \n# produced by the code in this document.\nif(!run_from_scratch) {\n    load(\"./uvlist_intermediate_products.Rds\")\n}\n```\n\n## Create the Raw Dataframe\n\nThe UVList data arrives as a list of dataframes, each scraped from one page of\nthe UVList site. Each table is a collection of game-by-system records. In other \nwords, they are structured such that games may have as many records as gaming \nsystems they are associated with. Our first step is gather these tables into a \nsingle R dataframe.\n\n```{r create_raw_df}\n# Pull the dataframe from each list and apply more appropriate (and R friendly)\n# column names.\ndf_collection <- lapply(table_collection, function(x) {\n    df <- x[[1]]\n    \n    names(df) <- c(\"raw_title\", \n                   \"publisher_developer\", \n                   \"release_year\", \n                   \"platform\")\n    \n    return(df)\n})\n\n# Test for empty dataframes.\nrow_counts <- sapply(df_collection, nrow)\n\n# Drop dataframe(s) with 0 rows. This should only occur if extra pages were\n# grabbed (my code intentionally grabs one page further than was initially\n# estimated).\ndf_collection <- df_collection[!(row_counts == 0)]\n\n# Insure that all columns are character to simplify cleaning.\ndf_collection <- lapply(df_collection, function(x) {\n    for(col_index in 1:ncol(x)) {\n        x[col_index] <- sapply(x[col_index], as.character)\n    }\n    \n    return(x)\n})\n\n# Collapse the results to a single dataframe.\nuv_raw_df <- as.data.frame(rbind_all(df_collection), stringsAsFactors = FALSE)\n\n# Clean intermediate products from our workspace.\nrm(row_counts, table_collection, df_collection)\n\n# Make a fresh dataframe copy to work with in case we need to appeal to the\n# original.\nuv_df <- uv_raw_df\n```\n\n## Inspection\n\nIn this section we do some spot checking of the data to get a sense of the \ncontents of each column and to look for obvious fixes.\n\n```{r inspection} \n# Examine the dataframe structure. \nstr(uv_df)\n\n# Observe the first few records. \nhead(uv_df)\n\n# Take a closer look at the print behavior of raw_titles. \nprint(head(uv_df$raw_title))\n\n# And for publisher_developer. \nprint(head(uv_df$publisher_developer))\n\n# Get a sense for the number of levels in our features. \napply(uv_df, 2, function(x) length(unique(x))) \n```\n\nWe observe a few things immedidately.\n\n* First, raw_title (at least) has some non-English characters encoded and has \nsome junk space after the text body.\n\n* Second, both raw\\_title and publisher\\_developer are complex fields - they \neach (sometimes) use parentheses to indicate separate kinds of information and \nsemi-colons to indicate multiple instances of the same kind of information. In \nraw_title, the content in the parentheses appears to be alternate translations \nand spellings. In publisher_developer, the publisher is outside of the \nparentheses and the developer in.\n\n* Third, as expected, there is some definite duplication in raw_title. This \nmakes sense given that we know some games will be associated with multiple \nsystems.\n\n* Finally, there are some junk space characters in our fields (e.g., \"\\t\" for \ntab). We'll want to clean out anything that is not a simply space.\n\nWe do some quick global cleaning before we proceed to the dirty work.\n\n```{r first_pass_global_cleaning} \n# Column values (especially game titles) may look strange if the encoding is not # specified. We insure that they will be properly represented as UTF-8. \nEncoding(uv_df$raw_title) <- \"UTF-8\" \nEncoding(uv_df$publisher_developer) <- \"UTF-8\" \nEncoding(uv_df$release_year) <- \"UTF-8\" \nEncoding(uv_df$platform) <- \"UTF-8\"\n\n# Clean out junk space characters.\njunk_regex <- \"((?! )[[:space:]])|(\\\\((?! )[[:space:]]\\\\))\"\n\nuv_df$raw_title <- gsub(junk_regex, \"\", uv_df$raw_title, perl = TRUE)\nuv_df$publisher_developer <- gsub(junk_regex, \"\", uv_df$publisher_developer, \n                                  perl = TRUE)\nuv_df$platform <- gsub(junk_regex, \"\", uv_df$platform, perl = TRUE)\n\nrm(junk_regex)\n\n# Many (if not all) of the titles have extra spaces on the head/tail for some \n#reason. We clean those up. \nuv_df$raw_title <- str_trim(uv_df$raw_title) \n\n# Some titles have a lead paren that they fail to close. We insure they close.\nuv_df$raw_title <- sapply(uv_df$raw_title, function(x) {\n    if(grepl(\" \\\\([^\\\\)]*$\", x)) {\n        return(paste0(x, \")\"))\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n```\n\n## Splitting the Complex Fields: raw\\_title and publisher\\_developer\n\nOur first order of business is breaking up our complex fields. In the case of\nraw\\_title, this will simplify our hunt for duplicate records and will make for\ncleaner records when it comes time to merge the UVList records with our other\nsources. In the case of publisher\\_developer, this is simply an essential step\nto pulling apart distinct pieces of information.\n\nWe start with raw\\_title.\n\n```{r raw_title_split}\n# As noted, there are a large number of titles with information added in parens\n# (e.g., \"title (stuff)\") after the title. This information usually appears to\n# be alternate spellings or title translations. We see how much of this we can\n# separate out.\n\n# The pattern for the added text is a body of parens preceded by a space and\n# for which the closing paren is the final character. There appears to be some\n# use of \";\" within the added text as a separator - we'll also want to replace\n# this with a more unique separator (we'll use \"----\").\n\n# We test for situations where our space-lead-paren pattern might be\n# problematic.\n\n# Added text doesn't finish the line? This could occur for records which have \n# parens as part of the title or in cases where the extra text was for some \n# reason inserted mid-title.\nhead(uv_df$raw_title[grepl(\"^.*\\\\).+$\", uv_df$raw_title)])\n\n# Parens without a leading space? This could occur for records that have parens \n# as part of the title or where catalogers failed to put the appropriate space \n# between title and extra text. We have no good way to distinguish such\n# instances automatically (e.g., tit(le) v. title(extra)).\nhead(uv_df$raw_title[grepl(\"[^ ]\\\\(\", uv_df$raw_title)])\n\n# Space, semi-colon, or semi-colon after paren that is inside a paren set? These\n# can't be distinguished from cases where the cataloger entered multiple bodies\n# of extra text (e.g., (extra (text) extra) v. (extra (text); extra) v. (extra)\n# (extra)).\nhead(uv_df$raw_title[grepl(\"\\\\)(?= |;|$)(?=.*\\\\))(?! \\\\()\", uv_df$raw_title, \n                           perl = TRUE)])\n\n# For the above cases, we're going to need to do some pre-processing of the\n# identified records before we can apply a single regex solution to parse\n# the title itself from the extra text.\n\n# We make a copy of the field so that we can keep the original text for\n# reference.\nraw_title <- uv_df$raw_title\n\n# We identify our problematic records (note: none of the records for the first\n# test - the text-after-parens test - were problematic for our regex).\nno_lead_space <- raw_title[grepl(\"[^ ]\\\\(\", raw_title)]\nbad_close_paren <- raw_title[grepl(\"\\\\)(?= |;|$)(?=.*\\\\))(?! \\\\()\", \n                                    raw_title, perl = TRUE)]\n\n# We manually inspect these collections to identify which raw_titles need\n# pre-processing to correct their formatting. \n\n# no_lead_space: Other than two special cases, these turn out to be all the \n# cases that would fail for our leading-paren pattern. We can fix these by\n# adding a space before the opening paren. You can use the regex patterns\n# in the line below to see what elements I remove from no_lead_space.\nno_lead_space <- no_lead_space[-grep(\"(^AMM)|(^hack)\", no_lead_space)]\nno_lead_space <- no_lead_space[!grepl(\" \\\\(.*?\\\\)(?= |$|;)\", \n                                      no_lead_space, perl = TRUE)]\n\n# bad_close_paren: These are all cases where a closing paren is eventually \n# followed by another closing paren without another opening paren (e.g., (extra \n# (text) extra) v. (extra (text)) (extra)). We can \"fix\" these by simply\n# replacing the problematic closing paren with a different symbol.\n\n# We correct the text for all problematic entries.\nraw_title <- sapply(raw_title, function(x) {\n    if(x %in% no_lead_space) {\n        x <- str_replace(x, \"\\\\(\", \" (\")\n    }\n    \n    if(x %in% bad_close_paren) {\n        x <- str_replace(x, \n                         \"\\\\)(?= |;|$)(?=.*\\\\))(?! \\\\()\",\n                         \"]\")\n    }\n    \n    return(x)\n}, USE.NAMES = FALSE)\n\n# Make a copy of our fixed-up raw_title that we can manipulate to extract the\n# extra text from.\nextra_text <- raw_title\n\n# We extract the extra text by pulling out all paren groups that are followed by\n# a space, end-of-line, or semi-colon. Because some titles can have multiple \n# extra text groups (e.g., (extra) (extra)), elements in the resulting list may\n# be larger than 1.\nextra_text <- str_extract_all(extra_text, \" \\\\(.*?\\\\)(?= |$|;)\")\n\n# Our extraction solution captures the leading space. We clean these out.\nextra_text <- lapply(extra_text, str_trim)\n\n# Titles without any extra text will produce an empty string. We make sure these\n# are flagged properly as NA.\nextra_text <- lapply(extra_text, function(x) {\n    if(length(x) == 0) {\n        return(NA)\n    } else {\n        return(x)\n    }\n})\n\n# At this point, we have something useful for getting to our trimmed titles: we \n# have a string (or strings) matching that which should be removed from the \n# raw_title string to leave just the trimmed title.\ntrim_title <- lapply(1:length(extra_text), function(index) {\n    \n    pd_string <- raw_title[index]\n    rd_list <- extra_text[[index]]\n    \n    for(rd_index in 1:length(rd_list)) {\n        if(is.na(rd_list[rd_index])) {\n            return(pd_string)\n        } else {\n        pd_string <- gsub(rd_list[rd_index], \"\", pd_string, fixed = TRUE)\n        }\n    }\n    \n    return(pd_string)\n})\n\n# We finish cleaning extra_text (we'll return to trim_title). First, our \n# extraction solution captures the leading and trailing paren. We delete these \n# off to leave just the text.\nextra_text <- lapply(extra_text, function(x) str_replace(x, \"^.\", \"\"))\nextra_text <- lapply(extra_text, function(x) str_replace(x, \".$\", \"\"))\n\n# Next, some games have multiple extra text entries inside the same set of \n# parens but separated by semi-colons (e.g., (extra; extra)). We split these up.\nextra_text <- str_split(extra_text, \";\")\n\n# Clean up any leading/trailing spaces leftover from the splitting.\nextra_text <- lapply(extra_text, str_trim)\n\n# Finally, at these point we have a list of variability sized character vectors \n# but we want a single character vector. For each collection of extracted extra \n# text, we collapse the collection to a single string. Then we collapse the \n# entire list to a single character vector.\nextra_text <- lapply(extra_text, function(x) paste(x, collapse = \"----\"))\nextra_text <- unlist(extra_text)\n\n# And back to trim_title. We simply need to clean up any leading/trailing spaces\n# and convert the list to a character vector.\ntrim_title <- lapply(trim_title, str_trim)\ntrim_title <- unlist(trim_title)\n\n# We still need to clean these for NAs, but we'll do that later when we do our\n# general NA cleaning. For now, we add our split columns to our dataframe.\n# Populate our table with our new columns.\nuv_df$trim_title <- trim_title\nuv_df$extra_text <- extra_text\n\n# Clean up our intermediate data products.\nrm(no_lead_space, bad_close_paren, raw_title, extra_text, trim_title)\n```\n\nWe perform a very similar process for splitting our publisher\\_developer field. \nFortunately, this field is slightly more consistent, allowing us to skip some of\nthe pre-processing work.\n\n```{r publisher_developer_split}\n# Similar to splitting raw_title, because there is a common pattern for\n# developer in the string (\"publisher (developer)\"), we can pull developer out\n# into it's own column. \n\n# We test for situations where our space-lead-paren pattern might be \n# problematic.\n\n# Added text doesn't finish the line?\nhead(uv_df$publisher_developer[grepl(\"^.*\\\\).+$\", uv_df$publisher_developer)])\n\n# Parens without a leading space?\nhead(uv_df$publisher_developer[grepl(\"[^ ]\\\\(\", uv_df$publisher_developer)])\n\n# Space, semi-colon, or semi-colon after paren that is inside a paren set? \nhead(uv_df$publisher_developer[grepl(\"\\\\)(?= |;|$)(?=.*\\\\))(?! \\\\()\", \n                                     uv_df$publisher_developer, \n                                     perl = TRUE)])\n\n# A unique concern here is that we might have cases where only a developer is \n# listed without a publisher. If catalogers follow convention, this will produce\n# records that start with a lead paren (i.e., which will not be detected because\n# no lead space).\nhead(uv_df$publisher_developer[grepl(\"^\\\\(\", uv_df$publisher_developer)])\n\n# Only our last test identified problematic records (those starting with a\n# lead-paren because only a developer is cataloged).\n\n# We make a copy of the field so that we can keep the original text for\n# reference.\npublisher_developer <- uv_df$publisher_developer\n\n# We fix our developer-only records by adding a leading space.\npublisher_developer <- sapply(publisher_developer, function(x) {\n    str_replace(x, \"^\\\\(\", \" (\")\n}, USE.NAMES = FALSE)\n\n# Make a copy of our fixed up publisher_developer that we can iterate on to\n# extract the developer.\ndeveloper <- publisher_developer\n\n# We extract the extra text by pulling out all paren groups that are followed by\n# a space, end-of-line, or semi-colon. Because some titles can have multiple \n# extra text groups (e.g., (extra) (extra)), elements in the resulting list may\n# be larger than 1.\ndeveloper <- str_extract_all(developer, \" \\\\(.*?\\\\)(?= |$|;)\")\n\n# Our extraction solution captures the leading space. We clean these out.\ndeveloper <- lapply(developer, str_trim)\n\n# publisher_developer entries without any developer will produce an empty\n# string. We make sure these are flagged properly as NA.\ndeveloper <- lapply(developer, function(x) {\n    if(length(x) == 0) {\n        return(NA)\n    } else {\n        return(x)\n    }\n})\n\n# At this point, we have something useful for getting to our publishers: we \n# have a string (or strings) matching that which should be removed from the \n# publisher_developer string to leave just the publisher.\npublisher <- lapply(1:length(developer), function(index) {\n    \n    pd_string <- publisher_developer[index]\n    rd_list <- developer[[index]]\n    \n    for(rd_index in 1:length(rd_list)) {\n        if(is.na(rd_list[rd_index])) {\n            return(pd_string)\n        } else {\n        pd_string <- gsub(rd_list[rd_index], \"\", pd_string, fixed = TRUE)\n        }\n    }\n    \n    return(pd_string)\n})\n\n# We finish cleaning developer (we'll return to publisher). First, our \n# extraction solution captures the leading and trailing paren. We delete these \n# off to leave just the text.\ndeveloper <- lapply(developer, function(x) str_replace(x, \"^.\", \"\"))\ndeveloper <- lapply(developer, function(x) str_replace(x, \".$\", \"\"))\n\n# Next, some games have multiple extra text entries inside the same set of \n# parens but separated by semi-colons (e.g., (extra; extra)). We split these up.\ndeveloper <- lapply(developer, function(x) unlist(str_split(x, \";\")))\n\n# Clean up any leading/trailing spaces leftover from the splitting.\ndeveloper <- lapply(developer, str_trim)\n\n# Finally, at these point we have a list of variability sized character vectors \n# but we want a single character vector. For each collection of extracted extra \n# text, we collapse the collection to a single string. Then we collapse the \n# entire list to a single character vector.\ndeveloper <- lapply(developer, function(x) paste(x, collapse = \"----\"))\ndeveloper <- unlist(developer)\n\n# And back to publisher. Multiple publishers are possible, so we do the same\n# split/clean/combine work.\npublisher <- str_split(publisher, \";\")\npublisher <- lapply(publisher, str_trim)\npublisher <- lapply(publisher, function(x) paste(x, collapse = \"----\"))\npublisher <- unlist(publisher)\n\n# We still need to clean these for NAs, but we'll do that later when we do our\n# general NA cleaning. For now, we add our split columns to our dataframe.\n# Populate our table with our new columns.\nuv_df$publisher <- publisher\nuv_df$developer <- developer\n\n# Clean up our intermediate data products.\nrm(publisher_developer, developer, publisher)\n```\n\n## Identify and Clean NAs, Set Types\n\nAt this point, we have columns that align with our target dataframe. We start \ngiving those columns a more thorough cleaning by standardizing how NAs are \nexpressed and insuring that column types are appropriate.\n\n```{r na_cleaning}\n## trim_title ##\n# First we examine trim_title for NA patterns. Because there are so many\n# unique values for trim_title, we can't examine all of them. \n\n# We simplify our problem by assuming that NA patterns are likely to be short.\n# We find counts of unique values and then look at just the shorter entries.\ntable(uv_df$trim_title[nchar(uv_df$trim_title) < 4], useNA = \"ifany\")\n\n# We also look for titles that start with unusual characters.\nuv_df$trim_title[grepl(\"^[^a-zA-Z0-9]\", uv_df$trim_title)]\n\n# One of the unusual values we observe is \"{TITLE}\". We take a closer look at\n# this entry.\nuv_df[grepl(\"{TITLE}\", uv_df$trim_title, fixed = TRUE), ]\n\n# The \"TITLE\" entry looks pretty bogus. We look for any possible variations on\n# \"TITLE\" that might suggest an entry with an unknown game.\nuv_df$trim_title[grepl(\"[Tt][Ii][Tt][Ll][Ee]\", uv_df$trim_title)]\n\n# \"Untitled RPG\" also looks pretty sketchy.\nuv_df[grepl(\"Untitled RPG\", uv_df$trim_title), ]\n\n# Based on the above inspections, our NA candidates are: \nna_candidates <- c(\"\", \"{TITLE}\", \"Untitled RPG\")\n\n# We replace any matching strings with NA.\nuv_df$trim_title <- ifelse(uv_df$trim_title %in% na_candidates, \n                           NA, uv_df$trim_title)\n\n# We don't really care about extra_text, but we do the courtesy of seeing if\n# there are obvious NAs.\ntable(uv_df$extra_text[nchar(uv_df$extra_text) < 4])\n\n# NA candidates are:\nna_candidates <- c(\"\", \"?\", \"???\", \"----\", \"NA\")\n\n# Replace our obvious NAs.\nuv_df$extra_text <- ifelse(uv_df$extra_text %in% na_candidates, \n                           NA, uv_df$extra_text)\n\n## publisher ##\ntable(uv_df$publisher[nchar(uv_df$publisher) < 4])\n\n# Candidate NAs: \nna_candidates <- c(\"\", \"-\", \"*\", \"?\", \"NA\")\n\n# Replace our obvious NAs.\nuv_df$publisher <- ifelse(uv_df$publisher %in% na_candidates, \n                          NA, uv_df$publisher)\n\n# Because multiple publishers are possible, it's possible that we might have\n# entries with a mix of valid and NA publishers. We inspect publishers for these\n# possible combinations.\nuv_df$publisher[grepl(\"^-\", uv_df$publisher)]\nuv_df$publisher[grepl(\"\\\\*\", uv_df$publisher)]\nuv_df$publisher[grepl(\"\\\\?\", uv_df$publisher)]\nuv_df$publisher[grepl(\"NA\", uv_df$publisher)]\n\n# We make some spot corrections for observed cases (based on as needed\n# research). Untouched names appear to be valid.\nuv_df$publisher[grepl(\"Mandriva\", uv_df$publisher)] <- \"Mandriva\"\nuv_df$publisher[grepl(\"Jam Creation\", uv_df$publisher)] <- \"Ainos\"\nuv_df$publisher[grepl(\"\\\\?{2,}\", uv_df$publisher)] <- NA\nuv_df$publisher <- ifelse(uv_df$publisher %in% c(\"G.J?\", \"Triple Eh?\"), \n               uv_df$publisher, gsub(\"\\\\?$\", \"\", uv_df$publisher))\n\n## developer ##\ntable(uv_df$developer[nchar(uv_df$developer) < 4])\n\n# Candidate NAs: \nna_candidates <- c(\"-\", \"*\", \"?\", \"NA\")\nuv_df$developer <- ifelse(uv_df$developer %in% na_candidates, \n                          NA, uv_df$developer)\n\n# Inspect for cases that may contain a mix of NA symbols.\nuv_df$developer[grepl(\"^-\", uv_df$developer)]\nuv_df$developer[grepl(\"\\\\*\", uv_df$developer)]\nuv_df$developer[grepl(\"\\\\?\", uv_df$developer)]\nuv_df$developer[grepl(\"NA\", uv_df$developer)]\n\n# Correct observed cases (based on as needed research). Untouched names\n# appear to be valid.\nuv_df$developer <- ifelse(uv_df$developer %in% c(\"G.J?\", \"Triple Eh?\"), \n                          uv_df$developer, gsub(\"\\\\?$\", \"\", uv_df$developer))\n\n## release_year ##\ntable(uv_df$release_year)\n\n# This is a pretty straightforward column to clean - any fields that are not\n# four numbers are problematic and we set to NA.\nuv_df$release_year <- sapply(uv_df$release_year, function(x) {\n    if(grepl(\"^[[:digit:]]{4}$\", x)) {\n        return(x)\n    } else {\n        return(NA)\n    }\n}, USE.NAMES = FALSE)\n\nuv_df$release_year <- as.integer(uv_df$release_year)\n\n## platform ##\ntable(uv_df$platform)\n\n# The only obvious NA candidate is the empty character string \"\".\nuv_df$platform <- ifelse(uv_df$platform == \"\", NA, uv_df$platform)\n\n# We clean up our intermediate data products.\nrm(na_candidates)\n```\n\n## Inspecting for String Mispellings and Variations\n\nWe're getting close to a clean dataset. The next major step is dealing with the\nfact that our records are game-platform records. We want to collapse the table\nto game records and merge our platform column to allow multi-way entries in the\nsame style as our publisher and developer fields.\n\nPrior to this, however, we want to ask an important question: Can we identify \nany entries in our key fields (trim\\_title, developer, publisher, release_year) \nthat are mispellings or which have unintended variability? Where mispellings or \nvariability occur, they will incorrectly give the appearance of a unique game \nwhen really the record is aligned with (or a duplicate of) other game records.\n\nDoing this type of cleaning completely requires some manual labor - at minimum\nwe would need to do enough manual cleaning to train a model for a more automated\nsolution. However, for the current project, we're going to automate what we can\nand - due to the sheer number of records - accept that we'll be left with some\nduplications.\n\nWe adopt a collection of strategies to to allow us to automate the removal of as\nmany duplicates as possible.\n\n1. Creating a column version with standardized casing and common differences \nremoved. Here common differences are those I observed during early passes for \nvariations/mispellings in the data (e.g., \"007: Nightfire\" v. \"007 -\nNightfire\").\n\n2. First just examine unique values and no the entire record. In other words,\ndon't worry about record duplication yet - just word about similarities among\nall the unique values in a single field (e.g., resolve similar titles alone).\n\n3. Automate the pairing of values with potential matches based on a simple \nstring similarity index. This allows us to identify smaller collections of term\nv. match terms that we can process with more precise rules.\n\n4. Standardize how matched candidates are evaluated.  This allows us to specify \nup front what the criteria are for an automated \"match\" (i.e., a potential \nmispelling/variation) or \"mismatch\" (i.e., false positive based on similarity\nalone). If we ever choose to do the proper manual cleaning of values, this will \nalso minimize the manual work required.\n\n### Standardize Casing and Known Variations\n\nBased on experience with both the UVList data and titling in general, there are\na few common sources of variation we can attempt to account for up front.\n\n* _Subtitles_: In the current data, catalogers varied a great deal as to\nwhether they used \" - \" or \": \" to indicate a subtitle.\n\n* _Hyphenation_: Whether or not a word is hyphenated (e.g., 2D v. 2-D) is also\na regular source of variation. Hyphenation is too flexible for us to catch\nall cases of correct or incorrect hyphenation, but we can at least standardize\nsome commonly abused cases.\n\n* _Common Shorthands_: We also keep an eye out for common shorthands that have\nvariabile forms (e.g., vs, vs., versus). Where we see these, we choose a single\nform to adopt.\n\n* _Spacing_: Although we cleaned up leading/trailing spaces, it is possible for\ncatalogers to inserted spaces into the title.\n\n* _Casing_: There are certaintly some games where the casing is part of the\nname (e.g., SiN). However, casing errors from human catalogers - especially\nfor situations where casing is non-standard - are likely to be a recurring\nsource of error.\n\n* _Roman and Written Numerals_: There are situations where coders chose to write\nnumbers as Roman numerals (e.g., 2 as \"II\") and to write short numbers as words\n(e.g., 1 as \"one\"). Where possible, we want to standardize how numbers are\npresented. Unfortunately, written numerals can be used in some unpredictable\nways, so we just focus on the Roman numeral to digit conversion.\n\n```{r variation_standardization}\n# Before we get started, we make new versions of the variables we'll be\n# tinkering with.\nuv_df$clean_title <- uv_df$trim_title\nuv_df$clean_publisher <- uv_df$publisher\nuv_df$clean_developer <- uv_df$developer\nuv_df$clean_platform <- uv_df$platform\n\n## Subtitles ##\n# Only really an issue for titles, so we ignore other fields during this\n# inspection.\n\n# Our ideal solution here is to convert either all \": \" to \" - \" or vice versa.\n# When picking between the two, it seems cleaner to convert all to \": \" (less\n# ambiguous) but we're more likely to succeed at converting all to \" - \"\n# (because we don't need to worry about identifying cases where \" - \" is not a\n# subtitle). We aim for the less ambiguous approach.\n\n# Key assumptions to this approach:\n# * \": \" is always used to indicate a subtitle.\n# * \" - \" is always used to indicate a subtitle.\n# * There are no incorrect uses of \":\" or \"-\" when indicating subtitles.\n\n# Secondary assumption:\n# * \": \" and \" - \" don't have unique uses in the same string.\n\n# \": \" used to indicate something other than a subtitle?\nhead(grep(\": \", unique(uv_df$clean_title), value = TRUE))\n# Result: None observed in the first few hundred entries.\n\n# \" - \" used to indicate something other than a subtitle?\nhead(grep(\" - \", unique(uv_df$clean_title), value = TRUE))\n# Result: Very few in titles and the only ones that were definitely problematic\n# to change were those in which a range of digits was being specified.\n\n# \" - \" used to indicate range of digits.\nlength(grep(\"[[:digit:]] - [[:digit:]]\", uv_df$clean_title, value = TRUE))\ngrep(\"[[:digit:]] - [[:digit:]]\", uv_df$clean_title, value = TRUE)\n# Result: We'll remove the spaces to disambiguate the problematic strings.\n\ntitles_to_fix <- c(\"Historyline 1914 - 1918\", \n                   \"Premier Manager 2004 - 2005\",\n                   \"Premier Manager 2005 - 2006\", \n                   \"Premier Manager 2006 - 2007\",\n                   \"UEFA Champions League 2004 - 2005\")\n\nuv_df$clean_title <- sapply(uv_df$clean_title, function(x) {\n    if(x %in% titles_to_fix) {\n        return(gsub(\" - \", \"-\", x))\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n\nrm(titles_to_fix)\n\n# \"-\" used incorrectly or unusually?\nhead(grep(\"[^ ]- | -[^ ]\", unique(uv_df$clean_title), value = TRUE))\n# Result: Yes for titles. These fall into three cases - situations where \"-\" is\n# used to surround a word/phrase as a subtitle (e.g., \"animals -puppy-\"), where\n# \"-\" is doubled up, and where \"-\" just used wrong. We handle each.\n\n# Surround a word/phrase.\nuv_df$clean_title <- sapply(uv_df$clean_title, function(x) {\n    if(grepl(\"(^| )-[^ ].*-\", x)) {\n        x <- sub(\" -\", \": \", x)\n        x <- gsub(\"(-(?= |$))|(^-)\", \"\", x, perl = TRUE)\n        return(x)\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n\n# Doubled up.\nuv_df$clean_title <- gsub(\"--\", \" - \", uv_df$clean_title)\n\n# Leading or trailing \"-\".\nuv_df$clean_title <- gsub(\" -(?=[^ ])\", \": \", uv_df$clean_title, perl = TRUE)\nuv_df$clean_title <- sapply(uv_df$clean_title, function(x) {\n    if(grepl(\"[^ ]- \", x, perl = TRUE)) {\n        return(gsub(\"- \", \": \", x))\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n\n# \":\" used incorrectly or unusually?\ngrep(\" :|[^ ]:[^ ]\", unique(uv_df$clean_title), value = TRUE)\n# Result: Yes for titles. The problematic strings fall into three groups:\n# legitimate uses of \":\" with digits, incorrect uses of \":\" with characters, and\n# extra leading space \" : \".\n\nuv_df$clean_title <- sapply(uv_df$clean_title, function(x) {\n    if(grepl(paste0(\"([[:alnum:]]:[[:alpha:]])\",\n                    \"|([[:alpha:]]:[[:alnum:]])\",\n                    \"|( : )|(::)|( :[^ ])\"), x)) {\n        return(gsub(\"(:(?=[^ :]))|( : )|([ ]*::)|( :)\", \": \", x, perl = TRUE))\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n\n# Converting \" - \" to \": \".\nuv_df$clean_title <- gsub(\" - \", \": \", uv_df$clean_title)\n\n## Hyphenation ##\n# This is also an issue we'll only really address in our title column. As noted\n# above, there are simply too many titles for us to correct all possible sources\n# of hyphenation. We simply scan for obvious variation issues and correct what\n# we can find.\nlength(grep(\"[^ ]-[^ ]\", uv_df$clean_title, value = TRUE))\nhead(grep(\"[^ ]-[^ ]\", uv_df$clean_title, value = TRUE), 100)\n\n# The most obvious problematic pattern is the shorthand for X-Dimensional (e.g.,\n# 2-D v. 2D v 2 D). We standardize these to remove the hyphenation.\nuv_df$clean_title <- sapply(uv_df$clean_title, function(x) {\n    if(grepl(\"([[:digit:]]-[Dd]( |$))\", x)) {\n        return(gsub(\"-(?=([Dd] )|([Dd]$))\", \"\", x, perl = TRUE))\n    } else {\n        return(x)\n    }\n}, USE.NAMES = FALSE)\n\n## Common Shorthands ##\n# We also try to standardize some other common patterns.\n# * vs, vs., versus\nuv_df$clean_title <- gsub(\"( vs )|( vs. )|( versus )\", \" vs \", uv_df$clean_title,\n                          ignore.case = TRUE)\n\n# * vol. , vol.\nuv_df$clean_title <- gsub(\"( vol )|( vol\\\\.)\", \" Volume \", uv_df$clean_title, \n                          ignore.case = TRUE)\n\n# * no.\nuv_df$clean_title <- gsub(\"( no\\\\. )|( no\\\\.)|( no [[:digit:]](?=(:|$)))\", \n                          \" Number \", \n                          uv_df$clean_title, ignore.case = TRUE, perl = TRUE)\n\n# * ep\nuv_df$clean_title <- gsub(\"( ep\\\\.)|( ep [[:digit:]])\", \" Number \",\n                          uv_df$clean_title, ignore.case = TRUE)\n\n# * &\nuv_df$clean_title <- gsub(\"[^[:punct:]]&[^[:punct:]]\", \" and \",\n                          uv_df$clean_title)\n\n## Spacing ##\n# This is one of our more straightforward cleaning steps. We simply don't allow\n# our strings to have more than a single space in them to remove accidental\n# multi-space sequences.\nuv_df$clean_title <- gsub(\"[ ]{2,}\", \" \", uv_df$clean_title)\nuv_df$clean_publisher <- gsub(\"[ ]{2,}\", \" \", uv_df$clean_publisher)\nuv_df$clean_developer <- gsub(\"[ ]{2,}\", \" \", uv_df$clean_developer)\nuv_df$clean_platform <- gsub(\"[ ]{2,}\", \" \", uv_df$clean_platform)\n\n# And we insure that we introduced no new leading/trailing spaces with all our\n# fixing.\nuv_df$clean_title <- str_trim(uv_df$clean_title)\nuv_df$clean_publisher <- str_trim(uv_df$clean_publisher)\nuv_df$clean_developer <- str_trim(uv_df$clean_developer)\nuv_df$clean_platform <- str_trim(uv_df$clean_platform)\n\n## Casing ##\n# Finally, we make the decision to ignore casing for our observed values and we\n# enforce standard casing behavior on all our target fields. This is a jump from\n# the title cleaning we've been doing and it breaks potentially desirable\n# formatting. We do this work in a new column with the understanding that we'll\n# need to eventually need to choose a desired \"natural\" casing for each value.\nuv_df$flat_title <- str_to_lower(uv_df$clean_title)\nuv_df$flat_publisher <- str_to_lower(uv_df$clean_publisher)\nuv_df$flat_developer <- str_to_lower(uv_df$clean_developer)\nuv_df$flat_platform <- str_to_lower(uv_df$clean_platform)\n\n## Roman and Written Numerals ##\n# We convert all characters chunks likely to be Roman numerals. The Roman\n# numeral conversion will result in some false positives (e.g., puppy and i\n# becomes puppy and 1), but we accept these for the purpose of string matching.\n# We restrict this work to titles as the choice of a number format is more\n# likely to be intentional for publishers, developers, and platforms.\n\n# Roman numeral conversion. Roman numerals almost always occur at the end of a\n# title/subtitle, so we minimize false positives by only looking for matches \n# indicating the end of a major title portion.\nnumeral_range <- c(1:50)\nfor(number in numeral_range) {\n    roman <- as.roman(number)\n    roman_regex <- paste0(\"(?<=[^[:alnum:]]|^)\", \n                          str_to_lower(roman), \n                          \"(?=[^[:alnum:]]|$)\")\n    uv_df$flat_title <- gsub(roman_regex, as.character(number), \n                            uv_df$flat_title, \n                            perl = TRUE)\n}\n\nrm(number, numeral_range, roman, roman_regex)\n\n# Written numeral conversion. We create a vector of common words and a matching\n# vector of their numeric form.\nwritten_numerals <- c(\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\",\n                      \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\",\n                      \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\",\n                      \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"forty\", \n                      \"fourty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\")\nnumeral_range <- c(0:20, 30, 40, 40, 50, 60, 70, 80, 90)\nfor(index in 1:length(numeral_range)) {\n    written_regex <- paste0(\"(?<=[^[:alnum:]]|^)\", \n                            written_numerals[index], \n                            \"(?=[^[:alnum:]]|$)\")\n    uv_df$flat_title <- gsub(written_regex, numeral_range[index], \n                            uv_df$flat_title, \n                            perl = TRUE)\n}\n\nrm(written_numerals, numeral_range, index, written_regex)\n```\n\n### Identify Variation/Mispelling Candidates\n\n**NOTE**: The `id_variations_from_scratch` global gets assessed in this section.\nIf set to `TRUE`, this section will take up to several hours to identify \ncandidate associations and then (in the next section) resolve the candidate \nassociations.\n\n```{r id_variation_candidates}\n# This section handles identifying candidates for variations and mispellings. It\n# is recommended that you only trigger this section if you are certain it is \n# necessary and know what you are doing as it is time-consuming.\nif(run_from_scratch) {\n    # First we look for variations/mispellings in our titles. This is the most\n    # time-consuming of the variation processing. We save some processing by\n    # focusing only on unique values (removing obvious duplicates from\n    # consideration).\n    unique_titles <- unique(uv_df$flat_title)\n    \n    title_matches <- fuzzy_match_all(unique_titles, max_dist = 0.1,\n                                     skip_pure_digit = TRUE, \n                                     min_test_length = 3,\n                                     assume_unique = TRUE, \n                                     remove_matches = TRUE)\n    \n    # Now for publisher... This takes a little bit of prep, as publisher is a \n    # complex field. We don't really want to evaluate the combinations of \n    # publishers, but the unique publisher names.\n    single_publishers <- unlist(str_split(uv_df$flat_publisher, \"----\"))\n    unique_publishers <- unique(single_publishers)\n    \n    publisher_matches <- fuzzy_match_all(unique_publishers, max_dist = 0.1,\n                                         skip_pure_digit = TRUE, \n                                         min_test_length = 2,\n                                         assume_unique = TRUE, \n                                         remove_matches = TRUE)\n    \n    # And developer...\n    single_developers <- unlist(str_split(uv_df$flat_developer, \"----\"))\n    unique_developers <- unique(uv_df$flat_developer)\n    \n    developer_matches <- fuzzy_match_all(unique_developers, max_dist = 0.1,\n                                         skip_pure_digit = TRUE, \n                                         min_test_length = 2,\n                                         assume_unique = TRUE, \n                                         remove_matches = TRUE)\n    \n    # And platform...\n    unique_platforms <- unique(uv_df$flat_platform)\n    \n    platform_matches <- fuzzy_match_all(unique_platforms, max_dist = 0.1,\n                                        skip_pure_digit = TRUE, \n                                        min_test_length = 2,\n                                        assume_unique = TRUE, \n                                        remove_matches = TRUE)\n    \n    # Remove intermediate data products.\n    rm(single_publishers, single_developers,\n       unique_titles, unique_publishers, unique_developers, unique_platforms)\n}\n```\n\n### Process Variation/Mispelling Candidates\n\nWe now have a set of lists that identify terms that appear to be reasonable \ncandidates for having variations/mispellings along with the associated candidate\nvariations/mispellings. We use a helper function to walk-through our target\nvariable matches (trim\\_title, publisher, developer, platform) and apply some \nbasic logic to auto-assess for match validity. See the definition for the\n`resolve_match` to observe the rules determing what counts as a match or not.\n\n**NOTE**: The `resolve_variations_from_scratch` global gets assessed in this\nsection. If set to `TRUE`, this section can take some time to complete.\n\n```{r process_variation_candidates}\n# This section handles processing candidates for variations and mispellings. It\n# is recommended that you only trigger this section if you are certain it is \n# necessary and know what you are doing as it is time-consuming.\nif(run_from_scratch) {\n    # We run our auto-resolver on each of the collections.\n    auto_title_matches <- resolve_all_match_sets(title_matches)\n    auto_publisher_matches <- resolve_all_match_sets(publisher_matches)\n    auto_developer_matches <- resolve_all_match_sets(developer_matches)\n    auto_platform_matches <- resolve_all_match_sets(platform_matches)\n}\n```\n\n### Clean Identified Variation/Mispelling Pairs\n\nAt this point we've produced some objects that could either be subjected to \nmanual processing (to resolve uncertain candidate matches) or which can be used\nto guide standardization of variation/mispelling collections as-is. For this \nproject, we simply use the current auto-matched results and acknowledge that \nwe've likely missed a few matches.\n\nIt's worth noting that we've done our matching with a highly stripped down \nversion of our values (certain punctuation and casing removed). We'll use the \nresults of matching these values to replace our richer-formatting titles as \nwell, but we'll retain the original raw versions to allow us to spot check for \nerrors if the need arises.\n\n```{r apply_variation_candidates}\nif(run_from_scratch) {\n    ## titles ##\n    fix_titles <- auto_title_matches$match_set_list\n    fix_titles <- fix_titles[auto_title_matches$auto_accept_index]\n    \n    clean_uv_df <- fix_all_match_sets(fix_titles, uv_df, \n                                      \"clean_title\", \"flat_title\")\n    \n    ## publishers ##\n    fix_publishers <- auto_publisher_matches$match_set_list\n    fix_publishers <- fix_publishers[auto_publisher_matches$auto_accept_index]\n    \n    clean_uv_df <- fix_all_match_sets(fix_publishers, clean_uv_df,\n                                      \"clean_publisher\", \"flat_publisher\")\n    \n    ## developers ##\n    fix_developers <- auto_developer_matches$match_set_list\n    fix_developers <- fix_developers[auto_developer_matches$auto_accept_index]\n    \n    clean_uv_df <- fix_all_match_sets(fix_developers, clean_uv_df,\n                                      \"clean_developer\", \"flat_developer\")\n    \n    ## platforms ##\n    # NO AUTO MATCHES\n    \n    ## cleanup ##\n    rm(fix_titles, fix_publishers, fix_developers)\n}\n```\n\n## Roll Up to One Record per Game\n\nAt last we're ready to head into the roll up. We want to produce a dataframe \nwith just one record per game. However, there is a reasonable concern that some \ndistinct games may share a title - either by happenstance or because we rolled \nthem together. We'll mitigate this risk by initially rolling up by both title \nand year and investigating any duplicates that remain to check for games that \nshouldn't be rolled up together.\n\nWe'll allow our other fields to initially become more complex, concatenating \nthem together. During this concatenation, we'll also sort the elements and\nremove duplicates.\n\nWhile we roll-up, we also assess whether we can distinguishing re-releases from \nnew games with the same name.\n\n```{r roll_up}\nif(run_from_scratch) {\n    # We collapse the dataframe by flat title and year.\n    gy_df <- clean_uv_df %>%\n        group_by(release_year, flat_title) %>%\n        summarise(\n            title = sort_elements(clean_title),\n            publisher = sort_elements(clean_publisher),\n            developer = sort_elements(clean_developer),\n            platform = sort_elements(clean_platform),\n            pub_test = sort_elements(flat_publisher, dedup = FALSE, \n                                     cf_split = \"~~~~\"),\n            dev_test = sort_elements(flat_developer, dedup = FALSE,\n                                     cf_split = \"~~~~\"),\n            plat_test = sort_elements(flat_platform, dedup = FALSE,\n                                      cf_split = \"~~~~\"),\n            flat_publisher = sort_elements(flat_publisher),\n            flat_developer = sort_elements(flat_developer),\n            flat_platform = sort_elements(flat_platform),\n            raw_title = sort_elements(raw_title)\n        )\n    \n    # And another version by title alone.\n    g_df <- clean_uv_df %>%\n        group_by(flat_title) %>%\n        summarise(\n            title = sort_elements(clean_title),\n            publisher = sort_elements(clean_publisher),\n            developer = sort_elements(clean_developer),\n            platform = sort_elements(clean_platform),\n            min_year = min(release_year, na.rm = TRUE),\n            max_year = max(release_year, na.rm = TRUE),\n            release_year = sort_elements(release_year),\n            pub_test = sort_elements(flat_publisher, dedup = FALSE, \n                                     cf_split = \"~~~~\"),\n            dev_test = sort_elements(flat_developer, dedup = FALSE,\n                                     cf_split = \"~~~~\"),\n            plat_test = sort_elements(flat_platform, dedup = FALSE,\n                                      cf_split = \"~~~~\"),\n            ry_test = sort_elements(release_year, dedup = FALSE,\n                                    cf_split = \"~~~~\"),\n            flat_publisher = sort_elements(flat_publisher),\n            flat_developer = sort_elements(flat_developer),\n            flat_platform = sort_elements(flat_platform),\n            raw_title = sort_elements(raw_title)\n        ) %>%\n        ungroup() %>%\n        mutate(max_min_gap = max_year - min_year,\n               njoins = nchar(gsub(\"[^-]\", \"\", release_year)) / 4)\n    \n    # We inspect the list for instances where the roll-up seems like it may be\n    # problematic: roll-ups that pulled together records with very different\n    # release years.\n    table(g_df$max_min_gap, useNA = \"ifany\")\n}\n```\n\nWe see that very few titles have duplicates beyond about a decade. External\nresearch also supports the idea that games sharing a name are relatively rare\nand tend to occur some time apart (e.g., \nhttp://www.giantbomb.com/same-name-different-game/3015-5521/). However, \nre-releases are quite common and often occur well after the game release (e.g.,\nmany of the games on gog.com).\n\nOur best solution would be to manually identify games that share a name and flag\nthem to avoid roll-up. Since that is not reasonable (for this version), we\nsimply acknowledge that some mistaken roll-ups definetely happened and move on.\n\nPending the quality of our other game sources, this is likely not too terrible \nan issue. Assuming we don't need to roll-up on title for at least one other rich\nsource, we should identify the later same-name games when we attempt to merge on\nname-title.\n\n## Finalize the Dataframe\n\nAt this point, we have all the pieces we want for our merge-ready dataframe. We\npull out the key columns and save the result.\n\n```{r merge_ready_df}\n# Select our target columns and create the merge-ready dataframe.\nuv_merge_df <- g_df %>%\n    select(\n        # Core features.\n        title, publisher, developer, platform, first_release_year = min_year,\n        # Merge friendly versions of character features.\n        flat_title, flat_publisher, flat_developer, flat_platform,\n        # Additional useful UVList features.\n        raw_title, all_release_year = release_year)\n\n# Add a prefix so that columns are not lost accidentally during the merge.\nnames(uv_merge_df) <- paste0(\"uv_\", names(uv_merge_df))\n\n# If run from scratch, we save the key intermediate and final data products.\nif(run_from_scratch) {\n    save(title_matches, publisher_matches, developer_matches,\n         platform_matches, auto_title_matches, auto_publisher_matches,\n         auto_developer_matches, auto_platform_matches,\n         g_df, gy_df, clean_uv_df,\n         file = \"./uvlist_intermediate_products.Rds\")\n    \n    save(uv_merge_df, file = \"./uv_merge_df.Rds\")\n}\n```\n",
    "created" : 1464293735613.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4031502031",
    "id" : "EEE163E8",
    "lastKnownWriteTime" : 1464293929,
    "path" : "C:/Projects/vgsample/vignettes/uvlist_data_cleaning.Rmd",
    "project_path" : "vignettes/uvlist_data_cleaning.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_markdown"
}